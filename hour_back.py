#hour_back.py
from flask import Flask, request, render_template, jsonify
from flask_cors import CORS
import os, json, time, re
from datetime import datetime, timedelta

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from bs4 import BeautifulSoup
import pandas as pd

app = Flask(__name__)
CORS(app)

# ====== ê¸°ì¤€ê°’ / ì„¤ì • ======
top30 = 168
avg_ref = 182.7
bottom70 = 194
START_DATE = os.environ.get("START_DATE", "2025-03-22")
MAX_DAYS   = int(os.environ.get("MAX_DAYS", "60"))

# ====== ìºì‹œ ë””ë ‰í† ë¦¬ ======
CACHE_DIR = os.environ.get("CACHE_DIR", "/data")
os.makedirs(CACHE_DIR, exist_ok=True)
RUNTIME_CACHE_FILE  = os.path.join(CACHE_DIR, "runtime_cache.json")
SCHEDULE_CACHE_FILE = os.path.join(CACHE_DIR, "schedule_index.json")

# ====== JSON ìœ í‹¸ ======
def _load_json(path, default):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return default
    return default

def _save_json(path, obj):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def get_runtime_cache():
    return _load_json(RUNTIME_CACHE_FILE, {})

def set_runtime_cache(key, runtime_min):
    cache = get_runtime_cache()
    cache[key] = {"runtime_min": runtime_min}
    _save_json(RUNTIME_CACHE_FILE, cache)

def get_schedule_cache():
    return _load_json(SCHEDULE_CACHE_FILE, {})

def set_schedule_cache_for_date(date_str, games_minimal_list):
    cache = get_schedule_cache()
    cache[date_str] = games_minimal_list
    _save_json(SCHEDULE_CACHE_FILE, cache)

def make_runtime_key(game_id: str, game_date: str) -> str:
    return f"{game_id}_{game_date}"

# ====== íŒ€ëª… ì •ê·œí™” ======
_ALIAS_MAP = {
    "SSG": ["SSG", "SSGëœë”ìŠ¤", "SSG Landers", "ëœë”ìŠ¤"],
    "KIA": ["KIA", "KIAíƒ€ì´ê±°ì¦ˆ", "ê¸°ì•„", "KIA Tigers", "íƒ€ì´ê±°ì¦ˆ"],
    "KT":  ["KT", "KTìœ„ì¦ˆ", "kt", "ì¼€ì´í‹°", "KT Wiz", "ìœ„ì¦ˆ"],
    "LG":  ["LG", "LGíŠ¸ìœˆìŠ¤", "ì—˜ì§€", "íŠ¸ìœˆìŠ¤"],
    "ë‘ì‚°": ["ë‘ì‚°", "ë‘ì‚°ë² ì–´ìŠ¤", "ë² ì–´ìŠ¤"],
    "ë¡¯ë°": ["ë¡¯ë°", "ë¡¯ë°ìì´ì–¸ì¸ ", "ìì´ì–¸ì¸ "],
    "ì‚¼ì„±": ["ì‚¼ì„±", "ì‚¼ì„±ë¼ì´ì˜¨ì¦ˆ", "ë¼ì´ì˜¨ì¦ˆ"],
    "NC":  ["NC", "NCë‹¤ì´ë…¸ìŠ¤", "ì—”ì”¨", "ë‹¤ì´ë…¸ìŠ¤"],
    "í‚¤ì›€": ["í‚¤ì›€", "í‚¤ì›€íˆì–´ë¡œì¦ˆ", "íˆì–´ë¡œì¦ˆ"],
    "í•œí™”": ["í•œí™”", "í•œí™”ì´ê¸€ìŠ¤", "ì´ê¸€ìŠ¤"],
}
_ALIAS_LOOKUP = {}
for canon, aliases in _ALIAS_MAP.items():
    for a in aliases:
        _ALIAS_LOOKUP[a.strip().lower()] = canon

def normalize_team(name: str) -> str | None:
    if not name:
        return None
    key = name.strip().lower()
    if key in _ALIAS_LOOKUP:
        return _ALIAS_LOOKUP[key]
    key2 = re.sub(r"\s+", "", key)
    return _ALIAS_LOOKUP.get(key2, name.strip())

# ====== Selenium ë“œë¼ì´ë²„ ======
def make_driver():
    options = Options()
    options.binary_location = os.environ.get("CHROME_BIN", "/usr/bin/google-chrome")
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,1200")
    options.add_argument("--lang=ko-KR")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36"
    )
    options.page_load_strategy = "eager"
    return webdriver.Chrome(options=options)

# ====== í¬ë¡¤ë§ ìœ í‹¸ ======
def get_today_cards(driver):
    wait = WebDriverWait(driver, 20)
    today = datetime.today().strftime("%Y%m%d")
    url = f"https://www.koreabaseball.com/Schedule/GameCenter/Main.aspx?gameDate={today}"
    driver.get(url)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div#contents")))
    time.sleep(0.6)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("li.game-cont") or soup.select("li[class*='game-cont']")

def extract_match_info_from_card(card_li):
    home_nm = card_li.get("home_nm")
    away_nm = card_li.get("away_nm")
    g_id = card_li.get("g_id")
    g_dt = card_li.get("g_dt")

    if not (home_nm and away_nm):
        home_alt = card_li.select_one(".team.home .emb img")
        away_alt = card_li.select_one(".team.away .emb img")
        if away_alt and not away_nm: away_nm = away_alt.get("alt", "").strip() or None
        if home_alt and not home_nm: home_nm = home_alt.get("alt", "").strip() or None

    if not (home_nm and away_nm):
        txt = card_li.get_text(" ", strip=True)
        m = re.search(r"([A-Za-zê°€-í£]+)\s*vs\s*([A-Za-zê°€-í£]+)", txt, re.I)
        if m:
            a, b = m.group(1), m.group(2)
            away_nm = away_nm or a
            home_nm = home_nm or b

    if not (g_id and g_dt):
        a = card_li.select_one("a[href*='GameCenter/Main.aspx'][href*='gameId='][href*='gameDate=']")
        if a and a.has_attr("href"):
            href = a["href"]
            gm = re.search(r"gameId=([A-Z0-9]+)", href)
            dm = re.search(r"gameDate=(\d{8})", href)
            if gm: g_id = g_id or gm.group(1)
            if dm: g_dt = g_dt or dm.group(1)

    return {"home": home_nm, "away": away_nm, "g_id": g_id, "g_dt": g_dt}

def find_today_matches_for_team(driver, my_team):
    my_canon = normalize_team(my_team)
    cards = get_today_cards(driver)
    results = []
    for li in cards:
        info = extract_match_info_from_card(li)
        h, a = info["home"], info["away"]
        if not (h and a):
            continue
        if my_canon in {normalize_team(h), normalize_team(a)}:
            rival_raw = h if normalize_team(a) == my_canon else a
            info["rival"] = normalize_team(rival_raw) or rival_raw  # <-- rivalë„ ì •ê·œí™”
            results.append(info)
    return results

def get_games_for_date(driver, date_str):
    cache = get_schedule_cache()
    if date_str in cache:
        return cache[date_str]

    wait = WebDriverWait(driver, 20)
    url = f"https://www.koreabaseball.com/Schedule/GameCenter/Main.aspx?gameDate={date_str}"
    driver.get(url)
    try:
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div#contents")))
    except Exception:
        set_schedule_cache_for_date(date_str, [])
        return []

    time.sleep(0.5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    cards = soup.select("li.game-cont") or soup.select("li[class*='game-cont']")
    games_minimal = []
    for li in cards:
        info = extract_match_info_from_card(li)
        if all([info.get("home"), info.get("away"), info.get("g_id"), info.get("g_dt")]):
            games_minimal.append({
                "home": info["home"],
                "away": info["away"],
                "g_id": info["g_id"],
                "g_dt": info["g_dt"],
            })

    set_schedule_cache_for_date(date_str, games_minimal)
    return games_minimal

def open_review_and_get_runtime(driver, game_id, game_date):
    today_str = datetime.today().strftime("%Y%m%d")
    use_cache = (game_date != today_str)
    key = make_runtime_key(game_id, game_date)

    if use_cache:
        rc = get_runtime_cache()
        hit = rc.get(key)
        if hit and isinstance(hit, dict) and "runtime_min" in hit:
            return hit["runtime_min"]

    wait = WebDriverWait(driver, 15)
    base = f"https://www.koreabaseball.com/Schedule/GameCenter/Main.aspx?gameId={game_id}&gameDate={game_date}"
    driver.get(base)
    try:
        review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(), 'ë¦¬ë·°')]")))
        review_tab.click()
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.record-etc")))
    except Exception:
        driver.get(base + "&section=REVIEW")
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.record-etc")))
        except Exception:
            pass

    time.sleep(0.5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    run_time_min = None
    record_etc = soup.select_one("div.record-etc")
    if record_etc:
        span = record_etc.select_one("span#txtRunTime")
        if span:
            runtime = span.get_text(strip=True)
            m = re.search(r"(\d{1,2})\s*[:ï¼š]\s*(\d{2})", runtime)
            if not m:
                m = re.search(r"(\d{1,2})\s*ì‹œê°„\s*(\d{1,2})\s*ë¶„", runtime)
            if m:
                h, mnt = int(m.group(1)), int(m.group(2))
                run_time_min = h * 60 + mnt

    if use_cache and run_time_min is not None:
        set_runtime_cache(key, run_time_min)

    return run_time_min

def collect_history_avg_runtime(my_team, rival_set, start_date=START_DATE):
    d = make_driver()
    today_minus_1 = (datetime.today() - timedelta(days=1)).strftime("%Y%m%d")
    dr = pd.date_range(start=start_date, end=today_minus_1)
    if len(dr) > MAX_DAYS:
        dr = dr[-MAX_DAYS:]
    date_list = [d.strftime("%Y%m%d") for d in dr]

    my_canon = normalize_team(my_team)
    rival_canon_set = {normalize_team(r) for r in (rival_set or set())} if rival_set else set()

    run_times = []
    for date in date_list:
        games = get_games_for_date(d, date)
        if not games:
            continue
        for info in games:
            home_raw, away_raw = info["home"], info["away"]
            home, away = normalize_team(home_raw), normalize_team(away_raw)

            if my_canon in {home, away}:
                opponent = home if away == my_canon else away
                if rival_canon_set and opponent not in rival_canon_set:
                    continue
                try:
                    rt = open_review_and_get_runtime(d, info["g_id"], info["g_dt"])
                except Exception:
                    rt = None
                if rt is not None:
                    run_times.append(rt)

    try: d.quit()
    except: pass

    if run_times:
        avg_time = round(sum(run_times) / len(run_times), 1)
        return avg_time, run_times
    else:
        return None, []

# ====== ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜ ======
def compute_for_team(team_name):
    if not team_name:
        return dict(
            result="íŒ€ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
            avg_time=None, css_class="", msg="",
            selected_team=None, top30=top30, avg_ref=avg_ref, bottom70=bottom70
        )

    d = make_driver()
    try:
        today_matches = find_today_matches_for_team(d, team_name)
    finally:
        try: d.quit()
        except: pass

    if not today_matches:
        return dict(
            result=f"{team_name}ì˜ ì˜¤ëŠ˜ ê²½ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            avg_time=None, css_class="", msg="",
            selected_team=team_name, top30=top30, avg_ref=avg_ref, bottom70=bottom70
        )

    # âœ… rivalì„ 'ì •ê·œí™”ëœ ì´ë¦„'ìœ¼ë¡œ ìˆ˜ì§‘ (í•µì‹¬ ìˆ˜ì •)
    rivals_today = {normalize_team(m["rival"]) for m in today_matches if m.get("rival")}
    rivals_str = ", ".join(sorted(rivals_today))

    try:
        # âœ… my_teamë„ ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ì „ë‹¬
        avg_time, _ = collect_history_avg_runtime(normalize_team(team_name), rivals_today)
    except Exception:
        avg_time = None

    css_class = ""; msg = ""
    if avg_time is not None:
        if avg_time < top30:
            css_class, msg = "fast", "ë¹ ë¥´ê²Œ ëë‚˜ëŠ” ê²½ê¸°ì…ë‹ˆë‹¤"
        elif avg_time < avg_ref:
            css_class, msg = "normal", "ì¼ë°˜ì ì¸ ê²½ê¸° ì†Œìš” ì‹œê°„ì…ë‹ˆë‹¤"
        elif avg_time < bottom70:
            css_class, msg = "bit-long", "ì¡°ê¸ˆ ê¸´ í¸ì´ì—ìš”"
        else:
            css_class, msg = "long", "ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë§¤ì¹˜ì—…ì…ë‹ˆë‹¤"
        result = f"ì˜¤ëŠ˜ {team_name}ì˜ ìƒëŒ€íŒ€ì€ {rivals_str}ì…ë‹ˆë‹¤.<br>ê³¼ê±° {team_name} vs {rivals_str} í‰ê·  ê²½ê¸°ì‹œê°„: {avg_time}ë¶„"
    else:
        result = f"ì˜¤ëŠ˜ {team_name}ì˜ ìƒëŒ€íŒ€ì€ {rivals_str}ì…ë‹ˆë‹¤.<br>ê³¼ê±° ê²½ê¸° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    return dict(
        result=result, avg_time=avg_time, css_class=css_class, msg=msg,
        selected_team=team_name, top30=top30, avg_ref=avg_ref, bottom70=bottom70
    )

# ====== ë¼ìš°íŠ¸ ======
@app.route("/", methods=["GET", "POST"])
@app.route("/hour", methods=["GET", "POST"])
def hour_index():
    try:
        team = (request.args.get("myteam") or request.form.get("myteam") or "").strip()
        ctx = compute_for_team(team) if team else dict(
            result=None, avg_time=None, css_class="", msg="",
            selected_team=None, top30=top30, avg_ref=avg_ref, bottom70=bottom70
        )
        return render_template("hour.html", **ctx)
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {str(e)}", 200

# ====== ì§„ë‹¨ ======
def _file_info(path):
    if not os.path.exists(path):
        return {"exists": False}
    st = os.stat(path)
    return {
        "exists": True,
        "size_bytes": st.st_size,
        "mtime": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(st.st_mtime)),
        "path": os.path.abspath(path),
    }

@app.route("/healthz")
def healthz():
    return "ok", 200

@app.route("/selenium/smoke")
def selenium_smoke():
    try:
        d = make_driver()
        d.get("about:blank")
        title = d.title or "blank"
        d.quit()
        return jsonify({"ok": True, "title": title})
    except Exception as e:
        try: d.quit()
        except: pass
        return jsonify({"ok": False, "error": f"{type(e).__name__}: {str(e)}"}), 500

@app.route("/cache/status")
def cache_status():
    return jsonify({
        "CACHE_DIR": os.path.abspath(CACHE_DIR),
        "runtime_cache": _file_info(RUNTIME_CACHE_FILE),
        "schedule_cache": _file_info(SCHEDULE_CACHE_FILE),
    })

@app.route("/cache/clear", methods=["POST"])
def cache_clear():
    deleted = []
    for p in [RUNTIME_CACHE_FILE, SCHEDULE_CACHE_FILE]:
        if os.path.exists(p):
            try:
                os.remove(p); deleted.append(os.path.basename(p))
            except Exception as e:
                return jsonify({"ok": False, "error": str(e)}), 500
    return jsonify({"ok": True, "deleted": deleted})

# =========================
# ğŸ” DEBUG ENDPOINTS
# - ìš´ì˜ ì¶œë ¥ì€ ë°”ê¾¸ì§€ ì•Šê³ , ì™œ "ê³¼ê±° ê²½ê¸° ë°ì´í„° ì—†ìŒ"ì´ ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì¤€ë‹¤
# =========================

def _norm(name: str) -> str:
    # í˜„ì¬ íŒŒì¼ì— normalize_teamì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ê³ , ì—†ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜
    try:
        return normalize_team(name)
    except NameError:
        return (name or "").strip()

@app.route("/debug/config")
def debug_config():
    return jsonify({
        "START_DATE": START_DATE,
        "MAX_DAYS": MAX_DAYS,
        "CACHE_DIR": CACHE_DIR,
    })

@app.route("/debug/date")
def debug_date():
    """
    íŠ¹ì • ë‚ ì§œì˜ ìŠ¤ì¼€ì¤„ ì¹´ë“œì—ì„œ ë‚˜ì˜¨ íŒ€ëª…/ì •ê·œí™”/ê²Œì„IDë¥¼ ë³´ì—¬ì¤Œ.
    ì˜ˆ) /debug/date?date=20250828
    """
    date_str = (request.args.get("date") or "").strip()
    if not (re.fullmatch(r"\d{8}", date_str)):
        return jsonify({"error": "date=YYYYMMDD í•„ìš”"}), 400

    d = make_driver()
    try:
        games = get_games_for_date(d, date_str)
    finally:
        try: d.quit()
        except: pass

    items = []
    for g in games:
        items.append({
            "home_raw": g.get("home"), "away_raw": g.get("away"),
            "home_norm": _norm(g.get("home")), "away_norm": _norm(g.get("away")),
            "game_id": g.get("g_id"), "game_date": g.get("g_dt")
        })
    return jsonify({"date": date_str, "games": items, "count": len(items)})

@app.route("/debug/today")
def debug_today():
    """
    ì˜¤ëŠ˜ í˜ì´ì§€ì—ì„œ íŒŒì‹±ëœ ì¹´ë“œ, ì •ê·œí™”, ê·¸ë¦¬ê³  'ì˜¤ëŠ˜ ìƒëŒ€(rivals_today)' ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì—¬ì¤Œ.
    ì˜ˆ) /debug/today?team=KIA
    """
    team = (request.args.get("team") or "").strip()
    if not team:
        return jsonify({"error":"team íŒŒë¼ë¯¸í„° í•„ìš”"}), 400

    d = make_driver()
    try:
        cards = get_today_cards(d)
        parsed = []
        for li in cards:
            info = extract_match_info_from_card(li)
            parsed.append({
                "home_raw": info.get("home"),
                "away_raw": info.get("away"),
                "home_norm": _norm(info.get("home")),
                "away_norm": _norm(info.get("away")),
                "g_id": info.get("g_id"),
                "g_dt": info.get("g_dt")
            })
        # compute rivals_today ê·¸ëŒ€ë¡œ ì¬í˜„
        my = _norm(team)
        rivals = set()
        for it in parsed:
            if my in {it["home_norm"], it["away_norm"]}:
                rival = it["home_norm"] if it["away_norm"] == my else it["away_norm"]
                rivals.add(rival)
    finally:
        try: d.quit()
        except: pass

    return jsonify({
        "team_input": team,
        "team_norm": my,
        "rivals_today": sorted(list(rivals)),
        "today_cards": parsed
    })

@app.route("/debug/review_runtime")
def debug_review_runtime():
    """
    íŠ¹ì • gameId/gameDateì—ì„œ ë¦¬ë·° íƒ­ ëŸ°íƒ€ì„ í…ìŠ¤íŠ¸/íŒŒì‹±ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì—¬ì¤Œ.
    ì˜ˆ) /debug/review_runtime?gameId=20240912LKKT0&gameDate=20250828
    """
    game_id = (request.args.get("gameId") or "").strip()
    game_date = (request.args.get("gameDate") or "").strip()
    if not game_id or not re.fullmatch(r"\d{8}", game_date or ""):
        return jsonify({"error":"gameId, gameDate=YYYYMMDD í•„ìš”"}), 400

    d = make_driver()
    try:
        # open_review_and_get_runtime ë¡œì§ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ, í…ìŠ¤íŠ¸ë„ ë…¸ì¶œ
        wait = WebDriverWait(d, 15)
        base = f"https://www.koreabaseball.com/Schedule/GameCenter/Main.aspx?gameId={game_id}&gameDate={game_date}"
        d.get(base)
        runtime_text = None
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(), 'ë¦¬ë·°')]")))
            review_tab.click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.record-etc")))
        except Exception:
            d.get(base + "&section=REVIEW")
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.record-etc")))
            except Exception:
                pass
        time.sleep(0.5)
        soup = BeautifulSoup(d.page_source, "html.parser")
        record_etc = soup.select_one("div.record-etc")
        if record_etc:
            span = record_etc.select_one("span#txtRunTime")
            if span:
                runtime_text = span.get_text(strip=True)
        parsed_min = None
        if runtime_text:
            m = re.search(r"(\d{1,2})\s*[:ï¼š]\s*(\d{2})", runtime_text)
            if not m:
                m = re.search(r"(\d{1,2})\s*ì‹œê°„\s*(\d{1,2})\s*ë¶„", runtime_text)
            if m:
                h, mn = int(m.group(1)), int(m.group(2))
                parsed_min = h*60 + mn
    finally:
        try: d.quit()
        except: pass

    return jsonify({
        "game_id": game_id, "game_date": game_date,
        "runtime_text": runtime_text, "parsed_minutes": parsed_min
    })

@app.route("/debug/history_reasons")
def debug_history_reasons():
    """
    ìˆ˜ì§‘ ë£¨í”„ ì „ì²´ì—ì„œ 'ì™œ ì œì™¸ë˜ì—ˆëŠ”ì§€'ë¥¼ ì´ìœ ë³„ë¡œ ì¹´ìš´íŠ¸/ìƒ˜í”Œ ì œê³µ.
    rival í•„í„°ë¥¼ ì£¼ê±°ë‚˜(ìƒëŒ€ ì§€ì •), ì•ˆ ì£¼ë©´ ì „ì²´ í‰ê·  ê¸°ì¤€ìœ¼ë¡œ ë™ì‘.
    ì˜ˆ) /debug/history_reasons?team=KIA&days=60
        /debug/history_reasons?team=KIA&rival=KT&days=60
    """
    team = (request.args.get("team") or "").strip()
    if not team:
        return jsonify({"error":"team íŒŒë¼ë¯¸í„° í•„ìš”"}), 400
    rival = (request.args.get("rival") or "").strip() or None
    try:
        days = int(request.args.get("days","60"))
    except:
        days = 60

    my = _norm(team)
    rival_norm = _norm(rival) if rival else None

    d = make_driver()
    try:
        end = (datetime.today() - timedelta(days=1)).strftime("%Y%m%d")
        start = (datetime.today() - timedelta(days=days)).strftime("%Y%m%d")
        dr = pd.date_range(start=start, end=end)

        counts = {
            "not_my_team": 0,
            "my_team_but_rival_mismatch": 0,
            "matched_but_no_runtime": 0,
            "matched_with_runtime": 0,
            "errors": 0
        }
        samples = {k: [] for k in counts}

        for dt in dr:
            date_str = dt.strftime("%Y%m%d")
            games = get_games_for_date(d, date_str)
            for g in games:
                try:
                    home_raw, away_raw = g["home"], g["away"]
                    home, away = _norm(home_raw), _norm(away_raw)

                    if my not in {home, away}:
                        counts["not_my_team"] += 1
                        if len(samples["not_my_team"]) < 6:
                            samples["not_my_team"].append({"date":date_str,"home":home_raw,"away":away_raw})
                        continue

                    opp = home if away == my else away
                    if rival_norm and opp != rival_norm:
                        counts["my_team_but_rival_mismatch"] += 1
                        if len(samples["my_team_but_rival_mismatch"]) < 6:
                            samples["my_team_but_rival_mismatch"].append({
                                "date":date_str,"home":home_raw,"away":away_raw,"opponent_norm":opp
                            })
                        continue

                    rt = open_review_and_get_runtime(d, g["g_id"], g["g_dt"])
                    if rt is None:
                        counts["matched_but_no_runtime"] += 1
                        if len(samples["matched_but_no_runtime"]) < 6:
                            samples["matched_but_no_runtime"].append({
                                "date":date_str,"home":home_raw,"away":away_raw,"game_id":g["g_id"]
                            })
                    else:
                        counts["matched_with_runtime"] += 1
                        if len(samples["matched_with_runtime"]) < 6:
                            samples["matched_with_runtime"].append({
                                "date":date_str,"home":home_raw,"away":away_raw,"runtime_min":rt
                            })
                except Exception as e:
                    counts["errors"] += 1
                    if len(samples["errors"]) < 6:
                        samples["errors"].append({"date":date_str,"error":f"{type(e).__name__}: {str(e)}"})
    finally:
        try: d.quit()
        except: pass

    return jsonify({
        "team_input": team, "team_norm": my, "rival_input": rival, "rival_norm": rival_norm,
        "days_scanned": days,
        "counts": counts,
        "sample_rows": samples
    })


if __name__ == "__main__":
    app.run(debug=True, port=5002, use_reloader=False)
